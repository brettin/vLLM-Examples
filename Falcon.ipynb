{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ff70af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NoneType: None\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import traceback\n",
    "traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdec7466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type RefinedWeb to instantiate a model of type falcon. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-21 08:22:33 llm_engine.py:72] Initializing an LLM engine with config: model='tiiuae/falcon-40b', tokenizer='tiiuae/falcon-40b', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.bfloat16, download_dir=None, load_format=auto, tensor_parallel_size=1, seed=0)\n",
      "NCCL version 2.14.3+cuda11.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory\n",
      "libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav34.so': /usr/lib/libibverbs/libmlx4-rdmav34.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 39.39 GiB total capacity; 1.76 GiB already allocated; 360.88 MiB free; 1.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m LLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiiuae/falcon-40b\u001b[39m\u001b[38;5;124m\"\u001b[39m,trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Name or path of your model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m output \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, my name is\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/entrypoints/llm.py:66\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, seed, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     56\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     57\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     58\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m LLMEngine\u001b[38;5;241m.\u001b[39mfrom_engine_args(engine_args)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/engine/llm_engine.py:223\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args)\u001b[0m\n\u001b[1;32m    220\u001b[0m distributed_init_method, placement_group \u001b[38;5;241m=\u001b[39m initialize_cluster(\n\u001b[1;32m    221\u001b[0m     parallel_config)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39mengine_configs,\n\u001b[1;32m    224\u001b[0m              distributed_init_method,\n\u001b[1;32m    225\u001b[0m              placement_group,\n\u001b[1;32m    226\u001b[0m              log_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m engine_args\u001b[38;5;241m.\u001b[39mdisable_log_stats)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/engine/llm_engine.py:102\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, distributed_init_method, placement_group, log_stats)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_workers_ray(placement_group)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_workers(distributed_init_method)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_cache()\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/engine/llm_engine.py:134\u001b[0m, in \u001b[0;36mLLMEngine._init_workers\u001b[0;34m(self, distributed_init_method)\u001b[0m\n\u001b[1;32m    126\u001b[0m worker \u001b[38;5;241m=\u001b[39m Worker(\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config,\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m     distributed_init_method,\n\u001b[1;32m    132\u001b[0m )\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\u001b[38;5;241m.\u001b[39mappend(worker)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    136\u001b[0m     get_all_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m )\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/engine/llm_engine.py:678\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, get_all_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    676\u001b[0m         executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(worker, method)\n\u001b[0;32m--> 678\u001b[0m     output \u001b[38;5;241m=\u001b[39m executor(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    679\u001b[0m     all_outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mworker_use_ray:\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/worker/worker.py:67\u001b[0m, in \u001b[0;36mWorker.init_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Initialize the model.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m set_random_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m get_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config)\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/model_executor/model_loader.py:58\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config)\u001b[0m\n\u001b[1;32m     54\u001b[0m model_class \u001b[38;5;241m=\u001b[39m _get_model_architecture(model_config\u001b[38;5;241m.\u001b[39mhf_config)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_default_torch_dtype(model_config\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Create a model instance.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# The weights will be initialized as empty tensors.\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class(model_config\u001b[38;5;241m.\u001b[39mhf_config)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mload_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdummy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     60\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/model_executor/models/falcon.py:386\u001b[0m, in \u001b[0;36mFalconForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m FalconModel(config)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m ColumnParallelLinear(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    388\u001b[0m                                     config\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    389\u001b[0m                                     bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    390\u001b[0m                                     gather_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    391\u001b[0m                                     perform_initialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler \u001b[38;5;241m=\u001b[39m Sampler(config\u001b[38;5;241m.\u001b[39mvocab_size)\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/model_executor/models/falcon.py:348\u001b[0m, in \u001b[0;36mFalconModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    345\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, perform_initialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# Transformer blocks\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    349\u001b[0m     FalconDecoderLayer(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    350\u001b[0m ])\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# Final Layer Norm\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m LayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/model_executor/models/falcon.py:349\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    345\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, perform_initialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# Transformer blocks\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m--> 349\u001b[0m     FalconDecoderLayer(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    350\u001b[0m ])\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# Final Layer Norm\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m LayerNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon)\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/model_executor/models/falcon.py:260\u001b[0m, in \u001b[0;36mFalconDecoderLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mnum_attention_heads\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention \u001b[38;5;241m=\u001b[39m FalconAttention(config)\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m FalconMLP(config)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mnew_decoder_architecture:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# The layer norm before self-attention\u001b[39;00m\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/model_executor/models/falcon.py:234\u001b[0m, in \u001b[0;36mFalconMLP.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mGELU()\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce_row_parallel_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m (config\u001b[38;5;241m.\u001b[39mnew_decoder_architecture\n\u001b[1;32m    233\u001b[0m                                         \u001b[38;5;129;01mor\u001b[39;00m config\u001b[38;5;241m.\u001b[39mparallel_attn)\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_4h_to_h \u001b[38;5;241m=\u001b[39m RowParallelLinear(\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m hidden_size,\n\u001b[1;32m    236\u001b[0m     hidden_size,\n\u001b[1;32m    237\u001b[0m     bias\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    238\u001b[0m     input_is_parallel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    239\u001b[0m     perform_initialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    240\u001b[0m     skip_bias_add\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m     reduce_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce_row_parallel_results)\n",
      "File \u001b[0;32m/rbscratch/brettin/conda_envs/vLLM/lib/python3.11/site-packages/vllm/model_executor/parallel_utils/tensor_parallel/layers.py:399\u001b[0m, in \u001b[0;36mRowParallelLinear.__init__\u001b[0;34m(self, input_size, output_size, bias, input_is_parallel, init_method, stride, keep_master_weight_for_test, skip_bias_add, params_dtype, use_cpu_initialization, perform_initialization, reduce_results)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaster_weight \u001b[38;5;241m=\u001b[39m _initialize_affine_weight_cpu(\n\u001b[1;32m    394\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size,\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size_per_partition, \u001b[38;5;241m1\u001b[39m, init_method,\n\u001b[1;32m    396\u001b[0m             stride\u001b[38;5;241m=\u001b[39mstride, return_master_weight\u001b[38;5;241m=\u001b[39mkeep_master_weight_for_test,\n\u001b[1;32m    397\u001b[0m             params_dtype\u001b[38;5;241m=\u001b[39mparams_dtype)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size_per_partition,\n\u001b[1;32m    401\u001b[0m         device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device(), dtype\u001b[38;5;241m=\u001b[39mparams_dtype))\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m perform_initialization:\n\u001b[1;32m    403\u001b[0m         _initialize_affine_weight_gpu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, init_method,\n\u001b[1;32m    404\u001b[0m                                       partition_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stride\u001b[38;5;241m=\u001b[39mstride)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 39.39 GiB total capacity; 1.76 GiB already allocated; 360.88 MiB free; 1.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"tiiuae/falcon-40b\",trust_remote_code=True)  # Name or path of your model\n",
    "output = llm.generate(\"Hello, my name is\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c159e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
