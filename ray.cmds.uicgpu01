export HTTP_PROXY="http://192.168.100.1:3128"
export HTTPS_PROXY="http://192.168.100.1:3128"
export http_proxy="http://192.168.100.1:3128"
export https_proxy="http://192.168.100.1:3128"

# set these to your cache to avoid permission problems
export HF_DATASETS_CACHE='/gpustor/brettin/.cache'
export TRANSFORMERS_CACHE='/gpustor/brettin/.cache'

# conda environment updated on 13 Mar 2024
conda activate /gpustor/brettin/anaconda3/envs/vLLM

# --num-gpus needs to be a minimum of 4 for Mixtral-8x7B-Instruct-v0.1
ray start --head --num-cpus 32 --num-gpus 4

# --tensor-parallel-size 4 should match ray --num-gpus if serving one model
nohup python -m vllm.entrypoints.api_server --model mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 4 --host localhost --port 8001 2>&1 > vllm.entrypoints.api_server.log &

# Watch the log for the api_server to start
tail -f vllm.entrypoints.api_server.log


# I have not found an easy way to stop the python command
ps -ef | grep python | grep entry
kill -9 pid

# to check the status of the ray server
ray status

# to stop the ray server
ray stop
