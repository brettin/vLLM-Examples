{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37b47182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcfcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaForCausalLM, CodeLlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba488dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-29 15:00:16 llm_engine.py:70] Initializing an LLM engine with config: model='codellama/CodeLlama-7b-hf', tokenizer='codellama/CodeLlama-7b-hf', tokenizer_mode=auto, trust_remote_code=False, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)\n",
      "INFO 08-29 15:00:16 tokenizer.py:29] For some LLaMA-based models, initializing the fast tokenizer may take a long time. To eliminate the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n",
      "NCCL version 2.14.3+cuda11.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libibverbs: Warning: couldn't load driver 'librxe-rdmav34.so': librxe-rdmav34.so: cannot open shared object file: No such file or directory\n",
      "libibverbs: Warning: couldn't load driver '/usr/lib/libibverbs/libmlx4-rdmav34.so': /usr/lib/libibverbs/libmlx4-rdmav34.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-29 15:01:14 llm_engine.py:196] # GPU blocks: 2862, # CPU blocks: 512\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"codellama/CodeLlama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bbd1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = '''def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL_ME>\n",
    "    return result\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30dd6625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "filling = llm.generate(PROMPT, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "105e3f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RequestOutput(request_id=0, prompt='def remove_non_ascii(s: str) -> str:\\n    \"\"\" <FILL_ME>\\n    return result\\n', prompt_token_ids=[1, 32007, 822, 3349, 29918, 5464, 29918, 294, 18869, 29898, 29879, 29901, 851, 29897, 1599, 851, 29901, 13, 1678, 9995, 29871, 32008, 13, 1678, 736, 1121, 13, 32009], outputs=[CompletionOutput(index=0, text='\\n    Given a string, return a new string where all non-ASCII', token_ids=[13, 1678, 11221, 263, 1347, 29892, 736, 263, 716, 1347, 988, 599, 1661, 29899, 28599, 2687], cumulative_logprob=-12.20508348941803, logprobs={}, finish_reason=length)], finished=True)]\n"
     ]
    }
   ],
   "source": [
    "print (filling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba8fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.transformers_utils import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db843abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-29 15:35:37 tokenizer.py:29] For some LLaMA-based models, initializing the fast tokenizer may take a long time. To eliminate the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n"
     ]
    }
   ],
   "source": [
    "VLLMtokenizer=tokenizer.get_tokenizer(\"hf-internal-testing/llama-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5aba21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('▁def', 'def')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.detokenize_incrementally(VLLMtokenizer,[\"\"], 822, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "24aef64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionOutput(index=0, text='\\n    Given a string, return a new string where all non-ASCII', token_ids=[13, 1678, 11221, 263, 1347, 29892, 736, 263, 716, 1347, 988, 599, 1661, 29899, 28599, 2687], cumulative_logprob=-12.20508348941803, logprobs={}, finish_reason=length)\n"
     ]
    }
   ],
   "source": [
    "print(filling[0].outputs[0].)\n",
    "prompty = filling[0].prompt_token_ids[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aa3fa1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def remove_non_ascii(s: str) -> str:\n",
      "    \"\"\" \n",
      "    return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output=\"\"\n",
    "for x in prompty:\n",
    "    if (x < 30000):\n",
    "        new_token, output = tokenizer.detokenize_incrementally(VLLMtokenizer,[output], x, 0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2beeaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfb76a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
