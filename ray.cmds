### Instructions for running falcon-180B-chat


### This works on a fresh login from uic-bastion.
### Start the ray cluster
source ./env.sh
conda activate vLLM
cd /gpustor/brettin/vLLM-Examples/
ray start --head  --num-cpus 64 --num-gpus 8

# Optional, to test that ray is properly running
cd ray-test
python ray-simple.py
cd ..

### Start the vllm server
python -m vllm.entrypoints.openai.api_server --model mistral-community/Mixtral-8x22B-v0.1 --tensor-parallel-size 8 --host localhost --max-model-len 2096 --enforce-eager --gpu-memory-utilization 0.98

nohup python -m vllm.entrypoints.openai.api_server --model mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 8 --host localhost 2>&1 > vllm.entrypoint.openai.log &
# nohup python -m vllm.entrypoints.api_server --model tiiuae/falcon-180B-chat --tensor-parallel-size 8 --host localhost 2>&1 > vllm.entrypoint.log &
# nohup python -m vllm.entrypoints.api_server --model mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 8 --host localhost 2>&1 > vllm.entrypoint.log &

# If successful, you will see this line when the server is ready- it takes a minute or so to load a model.
# INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)

### Run curl clients
# fresh dlogin from uic-bastion.

# https and http proxies need to be unset.
cd gpustor/vLLM-Examples/
rm *.log
time ./test.curl.simple 

# Look at the results.
cat *.log
